nohup: ignoring input
[Errno 2] No such file or directory: '/root/autodl-tmp/zc/graphqsat_deepgate/aigdata/train/METADATA'
No metadata available, that is fine for metadata generator.
ckt_net(
  (ckt_model): Model(
    (aggr_and_strc): TFMlpAggr()
    (aggr_not_strc): TFMlpAggr()
    (aggr_and_func): TFMlpAggr()
    (aggr_not_func): TFMlpAggr()
    (update_and_strc): GRU(128, 128)
    (update_and_func): GRU(128, 128)
    (update_not_strc): GRU(128, 128)
    (update_not_func): GRU(128, 128)
    (readout_prob): MLP(
      (fc): Sequential(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Dropout(p=0.2, inplace=False)
        (4): Linear(in_features=32, out_features=32, bias=True)
        (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU(inplace=True)
        (7): Dropout(p=0.2, inplace=False)
        (8): Linear(in_features=32, out_features=1, bias=True)
      )
    )
  )
  (mlp): MLP(
    (fc): Sequential(
      (0): Linear(in_features=256, out_features=32, bias=True)
      (1): ReLU(inplace=True)
      (2): Dropout(p=0.2, inplace=False)
      (3): Linear(in_features=32, out_features=32, bias=True)
      (4): ReLU(inplace=True)
      (5): Dropout(p=0.2, inplace=False)
      (6): Linear(in_features=32, out_features=32, bias=True)
      (7): ReLU(inplace=True)
      (8): Dropout(p=0.2, inplace=False)
      (9): Linear(in_features=32, out_features=32, bias=True)
      (10): ReLU(inplace=True)
      (11): Dropout(p=0.2, inplace=False)
      (12): Linear(in_features=32, out_features=32, bias=True)
      (13): ReLU(inplace=True)
      (14): Dropout(p=0.2, inplace=False)
      (15): Linear(in_features=32, out_features=2, bias=True)
    )
  )
)
Namespace(model_to_best_eval_path='./model/best_eval.pkl', save_freq=500, eps_init=1.0, eps_final=0.01, init_exploration_steps=5000, eps_decay_steps=30000, debug=False, eval_separately_on_each=True, eval_problems_paths='/root/autodl-tmp/zc/graphqsat_deepgate/aigdata/eval-problems-paths', eval_freq=1000, test_time_max_decisions_allowed=500, lr=2e-05, gamma=0.99, lr_scheduler_frequency=3000, lr_scheduler_gamma=1.0, step_freq=4, batch_size=64, penalty_size=0.1, aig_dir='/root/autodl-tmp/zc/graphqsat_deepgate/aigdata/train', cnf_dir='./cnf', tmp_dir='./tmp', target_update_freq=10, with_restarts=False, compare_with_restarts=False, train_max_time_decisions_allowed=500, buffer_size=20000, env_name='sat-v0', max_cap_fill_buffer=False, train_problems_paths='./aigdata/train', grad_clip=1.0, grad_clip_norm_type=2, batch_updates=50000, history_len=1, no_cuda=True, input_type='ckt', device=device(type='cpu'), logdir='runs/Jan11_14-46-41_autodl-container-7de8118d52-7f04756c')
Step of learner:  0
Episode 1: Return -24.900000000000084.
Step of learner:  0
Episode 2: Return -8.699999999999985.
Step of learner:  0
Episode 3: Return -5.399999999999997.
Step of learner:  0
Episode 4: Return -6.399999999999993.
Step of learner:  0
Episode 5: Return -11.099999999999977.
Step of learner:  0
Episode 6: Return -7.39999999999999.
Step of learner:  0
Episode 7: Return -2.3000000000000007.
Step of learner:  0
Episode 8: Return -8.399999999999986.
Step of learner:  0
Episode 9: Return -12.499999999999972.
Step of learner:  0
Episode 10: Return -3.2000000000000015.
Step of learner:  0
Episode 11: Return -5.899999999999995.
Step of learner:  0
Episode 12: Return -8.099999999999987.
Step of learner:  0
Episode 13: Return -3.0000000000000013.
Step of learner:  0
Episode 14: Return -10.09999999999998.
Step of learner:  0
Episode 15: Return -6.299999999999994.
Step of learner:  0
Episode 16: Return -1.6000000000000003.
Step of learner:  0
Episode 17: Return -7.999999999999988.
Step of learner:  0
Episode 18: Return -4.699999999999999.
Step of learner:  0
Episode 19: Return -2.600000000000001.
Step of learner:  0
Episode 20: Return -9.99999999999998.
Step of learner:  0
Episode 21: Return -12.499999999999972.
Step of learner:  0
Episode 22: Return -11.699999999999974.
Step of learner:  0
Episode 23: Return -11.599999999999975.
Step of learner:  0
Episode 24: Return -6.099999999999994.
Step of learner:  0
Episode 25: Return -9.799999999999981.
Step of learner:  0
Episode 26: Return -4.000000000000002.
Step of learner:  0
Episode 27: Return -1.4000000000000001.
Step of learner:  0
Episode 28: Return -8.799999999999985.
Step of learner:  0
Episode 29: Return -1.3.
Step of learner:  0
Episode 30: Return -10.09999999999998.
Step of learner:  0
Episode 31: Return -7.29999999999999.
Step of learner:  0
Episode 32: Return -11.899999999999974.
Step of learner:  0
Episode 33: Return -4.899999999999999.
Step of learner:  0
Episode 34: Return -3.5000000000000018.
Step of learner:  0
Episode 35: Return -7.699999999999989.
Step of learner:  0
Episode 36: Return -0.9999999999999999.
Step of learner:  0
Episode 37: Return -3.800000000000002.
Step of learner:  0
Episode 38: Return -2.9000000000000012.
Step of learner:  0
Episode 39: Return -1.0999999999999999.
Step of learner:  0
Episode 40: Return -18.29999999999999.
Step of learner:  0
Episode 41: Return -4.4.
Step of learner:  0
Episode 42: Return -15.099999999999962.
Step of learner:  0
Episode 43: Return -5.999999999999995.
Step of learner:  0
Episode 44: Return -10.799999999999978.
Step of learner:  0
Episode 45: Return -15.299999999999962.
Step of learner:  0
Episode 46: Return -3.4000000000000017.
Step of learner:  0
Episode 47: Return -11.699999999999974.
Step of learner:  0
Episode 48: Return -15.499999999999961.
Step of learner:  0
Episode 49: Return -12.199999999999973.
Step of learner:  0
Episode 50: Return -1.4000000000000001.
Step of learner:  0
Episode 51: Return -3.600000000000002.
Step of learner:  0
Episode 52: Return -13.799999999999967.
Step of learner:  0
Episode 53: Return -9.699999999999982.
Step of learner:  0
Episode 54: Return -4.899999999999999.
Step of learner:  0
Episode 55: Return -1.8000000000000005.
Step of learner:  0
Episode 56: Return -8.599999999999985.
Step of learner:  0
Episode 57: Return -2.0000000000000004.
Step of learner:  0
Episode 58: Return -20.900000000000027.
Step of learner:  0
Episode 59: Return -8.199999999999987.
Step of learner:  0
Episode 60: Return -1.4000000000000001.
Step of learner:  0
Episode 61: Return -12.99999999999997.
Step of learner:  0
Episode 62: Return -2.3000000000000007.
Step of learner:  0
Episode 63: Return -7.799999999999988.
Step of learner:  0
Episode 64: Return -0.9999999999999999.
Step of learner:  0
Episode 65: Return -1.7000000000000004.
Step of learner:  0
Episode 66: Return -4.300000000000001.
Step of learner:  0
/root/autodl-tmp/zc/graphqsat_deepgate/deepgate/utils/utils.py:206: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  hs[pi_node] = torch.tensor(pi_vec, dtype=torch.float)
/root/autodl-tmp/miniconda3/envs/dgsat/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Episode 67: Return -9.199999999999983.
Step of learner:  18
Episode 68: Return -7.699999999999989.
Step of learner:  37
Episode 69: Return -12.499999999999972.
Step of learner:  69
Episode 70: Return -9.099999999999984.
Step of learner:  92
Episode 71: Return -1.5000000000000002.
Step of learner:  96
Episode 72: Return -12.89999999999997.
Step of learner:  128
Episode 73: Return -7.899999999999988.
Step of learner:  148
Episode 74: Return -15.89999999999996.
Step of learner:  188
